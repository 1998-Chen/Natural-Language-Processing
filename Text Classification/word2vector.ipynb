{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 训练集 测试集\n",
    "train, test  = pd.read_csv(r'./Data/train.csv') , pd.read_csv(r'./Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载中文停用词\n",
    "stopword = pd.read_csv('stopword.csv').iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "a = '！'\n",
    "punc = string.punctuation\n",
    "if a in punc:\n",
    "    print('is punctuation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理：去除英文、数字和标点符号\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def process_text(sentence, stopword):\n",
    "    '''\n",
    "    单个句子的处理\n",
    "    '''\n",
    "    # re正则化去除非中文符号：返回str\n",
    "    relu = re.compile(r'[a-zA-Z0-9：，！,_.!+-=——,$%^，。？、~@#￥%……&*《》<>「」{}【】()/]')\n",
    "    sentence = relu.sub('', sentence)\n",
    "    # 去除中文停用词: 返回列表list\n",
    "    context = ' '.join(jieba.cut(sentence=sentence, cut_all=False)).split(' ')\n",
    "    for item in context:\n",
    "        if item in stopword:\n",
    "            context.remove(item)\n",
    "\n",
    "    return context\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_train = []\n",
    "    train_label = []\n",
    "    for i, j in enumerate(train.iloc[:,0]):\n",
    "        context = process_text(sentence = j,stopword = stopword)\n",
    "        all_train.append(context)\n",
    "        train_label.append(train.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'预告-凌晨245新浪视频播欧冠14决赛 曼联VS切尔西'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train[1000]\n",
    "train_label[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文词表\n",
    "\n",
    "def word_dict(data):\n",
    "\n",
    "    word_dict = {    }\n",
    "    word_split = []\n",
    "    for item in data:\n",
    "        item = ' '.join(jieba.cut(item, cut_all=False)).split(' ')\n",
    "        # stopword\n",
    "        for ind in item:\n",
    "            if ind not in stopword:\n",
    "\n",
    "        \n",
    "        word_split.append([item])\n",
    "    return word_split\n",
    "\n",
    "word_split = word_cut(train.iloc[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['两天 价 网站 背后 重重 迷雾 ： 做个 网站 究竟 要 多少 钱']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(context, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '女子' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\Master\\Nature Language Process\\Torchtext.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Master/Nature%20Language%20Process/Torchtext.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mwv[\u001b[39m'\u001b[39;49m\u001b[39m女子\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[1;32mf:\\python\\miniconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:395\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[0;32m    397\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32mf:\\python\\miniconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:438\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    415\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \n\u001b[0;32m    417\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[0;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32mf:\\python\\miniconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[0;32m    411\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key '女子' not present\""
     ]
    }
   ],
   "source": [
    "model.wv['女子']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 329kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.0kB/s]\n",
      "Downloading: 100%|██████████| 624/624 [00:00<00:00, 624kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '深度强化学习是目前金融决策的研究热点，我们需要关注研究热点方向，并尝试'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 3918, 2428, 2487, 1265, 2110,  739, 3221, 4680, 1184, 7032,\n",
       "       6084, 1104, 5032, 4638, 4777, 4955, 4178, 4157, 8024, 2769,  812,\n",
       "       7444, 6206, 1068, 3800, 4777, 4955, 4178, 4157, 3175, 1403, 8024,\n",
       "       2400, 2214, 6407,  102])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.504 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'深度 强化 化学 学习 是 目前 金融 决策 的 研究 热点 ， 我们 需要 关注 研究 热点 方向 ， 并 尝试'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "' '.join(jieba.cut(sentence, cut_all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ddc83c648adabb5957e95c6c28d8e2506149ada91768f4c667c962aea010f01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
